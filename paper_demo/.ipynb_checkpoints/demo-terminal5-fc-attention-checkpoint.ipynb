{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn import datasets, linear_model\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import BatchNormalization, Input, Embedding, Concatenate, Conv1D, MaxPooling1D, Flatten, merge\n",
    "from keras.layers import merge, Concatenate, Permute, RepeatVector, Reshape\n",
    "from keras.models import Sequential, Model\n",
    "import keras.backend as K\n",
    "import statsmodels.formula.api as smf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# prevent tensorflow from allocating the entire GPU memory at once\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth=True\n",
    "# sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOBAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAGS = 10\n",
    "sel = [5,7] # weather features to use\n",
    "sel2 = [0,1,2,7] # eventlags featurs to use\n",
    "\n",
    "# word embeddings parameters\n",
    "#GLOVE_DIR = \"/home/fmpr/datasets/glove.6B/\"\n",
    "GLOVE_DIR = \"/mnt/sdb1/datasets/glove.6B/\"\n",
    "MAX_SEQUENCE_LENGTH = 350 #600\n",
    "MAX_NB_WORDS = 600 #5000\n",
    "EMBEDDING_DIM = 300 #300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weather data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_temp</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_gust</th>\n",
       "      <th>visibility</th>\n",
       "      <th>pressure</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>snow_depth</th>\n",
       "      <th>fog</th>\n",
       "      <th>rain_drizzle</th>\n",
       "      <th>snow_ice</th>\n",
       "      <th>thunder</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-01</th>\n",
       "      <td>-2.016735</td>\n",
       "      <td>-2.128488</td>\n",
       "      <td>2.946112</td>\n",
       "      <td>2.512634</td>\n",
       "      <td>0.666065</td>\n",
       "      <td>0.571023</td>\n",
       "      <td>-0.273606</td>\n",
       "      <td>-0.240596</td>\n",
       "      <td>-0.326133</td>\n",
       "      <td>-0.709709</td>\n",
       "      <td>-0.255411</td>\n",
       "      <td>-0.037018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-02</th>\n",
       "      <td>-1.602221</td>\n",
       "      <td>-1.739134</td>\n",
       "      <td>0.276908</td>\n",
       "      <td>-0.188044</td>\n",
       "      <td>0.236614</td>\n",
       "      <td>-0.040850</td>\n",
       "      <td>-0.378683</td>\n",
       "      <td>-0.240596</td>\n",
       "      <td>-0.326133</td>\n",
       "      <td>-0.709709</td>\n",
       "      <td>3.913921</td>\n",
       "      <td>-0.037018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-03</th>\n",
       "      <td>-1.108462</td>\n",
       "      <td>-1.472148</td>\n",
       "      <td>1.211129</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.666065</td>\n",
       "      <td>-0.503153</td>\n",
       "      <td>-0.378683</td>\n",
       "      <td>-0.240596</td>\n",
       "      <td>-0.326133</td>\n",
       "      <td>-0.709709</td>\n",
       "      <td>-0.255411</td>\n",
       "      <td>-0.037018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-04</th>\n",
       "      <td>-1.413252</td>\n",
       "      <td>-1.238535</td>\n",
       "      <td>1.033182</td>\n",
       "      <td>-0.579745</td>\n",
       "      <td>0.666065</td>\n",
       "      <td>0.244691</td>\n",
       "      <td>-0.378683</td>\n",
       "      <td>-0.240596</td>\n",
       "      <td>-0.326133</td>\n",
       "      <td>-0.709709</td>\n",
       "      <td>-0.255411</td>\n",
       "      <td>-0.037018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>-0.626895</td>\n",
       "      <td>-1.188475</td>\n",
       "      <td>0.588315</td>\n",
       "      <td>-1.218837</td>\n",
       "      <td>0.666065</td>\n",
       "      <td>-0.272002</td>\n",
       "      <td>-0.378683</td>\n",
       "      <td>-0.240596</td>\n",
       "      <td>-0.326133</td>\n",
       "      <td>-0.709709</td>\n",
       "      <td>-0.255411</td>\n",
       "      <td>-0.037018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-27</th>\n",
       "      <td>-0.919493</td>\n",
       "      <td>-0.237338</td>\n",
       "      <td>0.677289</td>\n",
       "      <td>1.852927</td>\n",
       "      <td>0.482014</td>\n",
       "      <td>-0.394376</td>\n",
       "      <td>-0.326145</td>\n",
       "      <td>-0.240596</td>\n",
       "      <td>-0.326133</td>\n",
       "      <td>-0.709709</td>\n",
       "      <td>-0.255411</td>\n",
       "      <td>-0.037018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28</th>\n",
       "      <td>-0.742715</td>\n",
       "      <td>-0.237338</td>\n",
       "      <td>0.098961</td>\n",
       "      <td>-0.579745</td>\n",
       "      <td>0.666065</td>\n",
       "      <td>0.122316</td>\n",
       "      <td>-0.378683</td>\n",
       "      <td>-0.240596</td>\n",
       "      <td>-0.326133</td>\n",
       "      <td>-0.709709</td>\n",
       "      <td>-0.255411</td>\n",
       "      <td>-0.037018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29</th>\n",
       "      <td>-0.864630</td>\n",
       "      <td>-0.999360</td>\n",
       "      <td>-0.701800</td>\n",
       "      <td>-0.373587</td>\n",
       "      <td>-0.560938</td>\n",
       "      <td>-0.639125</td>\n",
       "      <td>-0.378683</td>\n",
       "      <td>-0.240596</td>\n",
       "      <td>-0.326133</td>\n",
       "      <td>1.408545</td>\n",
       "      <td>-0.255411</td>\n",
       "      <td>-0.037018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>-0.919493</td>\n",
       "      <td>-1.021609</td>\n",
       "      <td>1.166643</td>\n",
       "      <td>1.069524</td>\n",
       "      <td>0.604715</td>\n",
       "      <td>-1.781287</td>\n",
       "      <td>0.645816</td>\n",
       "      <td>-0.240596</td>\n",
       "      <td>3.065185</td>\n",
       "      <td>-0.709709</td>\n",
       "      <td>3.913921</td>\n",
       "      <td>-0.037018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>-1.053600</td>\n",
       "      <td>-1.127291</td>\n",
       "      <td>0.632802</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.666065</td>\n",
       "      <td>0.067928</td>\n",
       "      <td>-0.352414</td>\n",
       "      <td>-0.240596</td>\n",
       "      <td>-0.326133</td>\n",
       "      <td>-0.709709</td>\n",
       "      <td>-0.255411</td>\n",
       "      <td>-0.037018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2922 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            min_temp  max_temp  wind_speed  wind_gust  visibility  pressure  \\\n",
       "date                                                                          \n",
       "2009-01-01 -2.016735 -2.128488    2.946112   2.512634    0.666065  0.571023   \n",
       "2009-01-02 -1.602221 -1.739134    0.276908  -0.188044    0.236614 -0.040850   \n",
       "2009-01-03 -1.108462 -1.472148    1.211129   0.018115    0.666065 -0.503153   \n",
       "2009-01-04 -1.413252 -1.238535    1.033182  -0.579745    0.666065  0.244691   \n",
       "2009-01-05 -0.626895 -1.188475    0.588315  -1.218837    0.666065 -0.272002   \n",
       "...              ...       ...         ...        ...         ...       ...   \n",
       "2016-12-27 -0.919493 -0.237338    0.677289   1.852927    0.482014 -0.394376   \n",
       "2016-12-28 -0.742715 -0.237338    0.098961  -0.579745    0.666065  0.122316   \n",
       "2016-12-29 -0.864630 -0.999360   -0.701800  -0.373587   -0.560938 -0.639125   \n",
       "2016-12-30 -0.919493 -1.021609    1.166643   1.069524    0.604715 -1.781287   \n",
       "2016-12-31 -1.053600 -1.127291    0.632802   0.018115    0.666065  0.067928   \n",
       "\n",
       "            precipitation  snow_depth       fog  rain_drizzle  snow_ice  \\\n",
       "date                                                                      \n",
       "2009-01-01      -0.273606   -0.240596 -0.326133     -0.709709 -0.255411   \n",
       "2009-01-02      -0.378683   -0.240596 -0.326133     -0.709709  3.913921   \n",
       "2009-01-03      -0.378683   -0.240596 -0.326133     -0.709709 -0.255411   \n",
       "2009-01-04      -0.378683   -0.240596 -0.326133     -0.709709 -0.255411   \n",
       "2009-01-05      -0.378683   -0.240596 -0.326133     -0.709709 -0.255411   \n",
       "...                   ...         ...       ...           ...       ...   \n",
       "2016-12-27      -0.326145   -0.240596 -0.326133     -0.709709 -0.255411   \n",
       "2016-12-28      -0.378683   -0.240596 -0.326133     -0.709709 -0.255411   \n",
       "2016-12-29      -0.378683   -0.240596 -0.326133      1.408545 -0.255411   \n",
       "2016-12-30       0.645816   -0.240596  3.065185     -0.709709  3.913921   \n",
       "2016-12-31      -0.352414   -0.240596 -0.326133     -0.709709 -0.255411   \n",
       "\n",
       "             thunder  \n",
       "date                  \n",
       "2009-01-01 -0.037018  \n",
       "2009-01-02 -0.037018  \n",
       "2009-01-03 -0.037018  \n",
       "2009-01-04 -0.037018  \n",
       "2009-01-05 -0.037018  \n",
       "...              ...  \n",
       "2016-12-27 -0.037018  \n",
       "2016-12-28 -0.037018  \n",
       "2016-12-29 -0.037018  \n",
       "2016-12-30 -0.037018  \n",
       "2016-12-31 -0.037018  \n",
       "\n",
       "[2922 rows x 12 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"loading weather data...\")\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(\"central_park_weather.csv\")\n",
    "df = df.set_index(\"date\")\n",
    "df.index = pd.to_datetime(df.index, format='%Y-%m-%d')\n",
    "\n",
    "# replace predefined values with NaN\n",
    "df = df.replace(99.99, np.nan)\n",
    "df = df.replace(999.9, np.nan)\n",
    "df = df.replace(9999.9, np.nan)\n",
    "\n",
    "# replace NaN with 0 for snow depth\n",
    "df[\"snow_depth\"] = df[\"snow_depth\"].fillna(0)\n",
    "\n",
    "# do interpolation for the remaining NaNs\n",
    "df = df.interpolate()\n",
    "\n",
    "# standardize data\n",
    "removed_mean = df.mean()\n",
    "removed_std = df.std()\n",
    "weather = (df - removed_mean) / removed_std\n",
    "weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading events data...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading events data...\")\n",
    "\n",
    "events = pd.read_csv(\"terminal5_events_preprocessed.tsv\", sep=\"\\t\")\n",
    "events.head()\n",
    "\n",
    "events['start_time'] = pd.to_datetime(events['start_time'], format='%Y-%m-%d %H:%M')\n",
    "events['date'] = events['start_time'].dt.strftime(\"%Y-%m-%d\")\n",
    "events = events[[\"date\",\"start_time\",\"title\",\"url\",\"description\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load taxi data (and merge with others and detrend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading taxi data (and merging and detrending)...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading taxi data (and merging and detrending)...\")\n",
    "\n",
    "df = pd.read_csv(\"pickups_terminal_5_0.003.csv\")\n",
    "\n",
    "df_sum = pd.DataFrame(df.groupby(\"date\")[\"pickups\"].sum())\n",
    "df_sum[\"date\"] = df_sum.index\n",
    "df_sum.index = pd.to_datetime(df_sum.index, format='%Y-%m-%d %H:%M')\n",
    "df_sum[\"dow\"] = df_sum.index.weekday\n",
    "\n",
    "# add events information\n",
    "event_col = np.zeros((len(df_sum)))\n",
    "late_event = np.zeros((len(df_sum)))\n",
    "really_late_event = np.zeros((len(df_sum)))\n",
    "event_desc_col = []\n",
    "for i in range(len(df_sum)):\n",
    "    if df_sum.iloc[i].date in events[\"date\"].values:\n",
    "        event_col[i] = 1\n",
    "        event_descr = \"\"\n",
    "        for e in events[events.date == df_sum.iloc[i].date][\"description\"]:\n",
    "            event_descr += str(e) + \" \"\n",
    "        event_desc_col.append(event_descr)\n",
    "        for e in events[events.date == df_sum.iloc[i].date][\"start_time\"]:\n",
    "            if e.hour >= 20:\n",
    "                late_event[i] = 1\n",
    "            if e.hour >= 21:\n",
    "                really_late_event[i] = 1\n",
    "    else:\n",
    "        event_desc_col.append(\"None\")\n",
    "\n",
    "df_sum[\"event\"] = event_col\n",
    "df_sum[\"late_event\"] = late_event\n",
    "df_sum[\"really_late_event\"] = really_late_event\n",
    "df_sum[\"event_desc\"] = event_desc_col\n",
    "df_sum[\"event_next_day\"] = pd.Series(df_sum[\"event\"]).shift(-1)\n",
    "df_sum[\"late_event_next_day\"] = pd.Series(df_sum[\"late_event\"]).shift(-1)\n",
    "df_sum[\"really_late_event_next_day\"] = pd.Series(df_sum[\"really_late_event\"]).shift(-1)\n",
    "df_sum[\"event_next_day_desc\"] = pd.Series(df_sum[\"event_desc\"]).shift(-1)\n",
    "\n",
    "# merge with weather data\n",
    "\n",
    "df_sum = df_sum.rename({'date': 'date_col'}, axis=1)\n",
    "df_sum = df_sum.join(weather, how=\"inner\", on=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only data after 2013\n",
    "START_YEAR = 2013\n",
    "df_sum = df_sum.loc[df_sum.index.year >= START_YEAR]\n",
    "df_sum.head()\n",
    "\n",
    "df_sum[\"year\"] = df_sum.index.year\n",
    "\n",
    "trend_mean = df_sum[df_sum.index.year < 2015].groupby([\"dow\"]).mean()[\"pickups\"]\n",
    "trend_std = df_sum[\"pickups\"].std()\n",
    "\n",
    "# build vectors with trend to remove and std\n",
    "trend = []\n",
    "std = []\n",
    "for ix, row in df_sum.iterrows():\n",
    "    trend.append(trend_mean[row.dow])\n",
    "    std.append(trend_std)\n",
    "\n",
    "df_sum[\"trend\"] = trend\n",
    "df_sum[\"std\"] = std\n",
    "\n",
    "# detrend data\n",
    "df_sum[\"detrended\"] = (df_sum[\"pickups\"] - df_sum[\"trend\"]) / df_sum[\"std\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build lags and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building lags...\n"
     ]
    }
   ],
   "source": [
    "print(\"building lags...\")\n",
    "\n",
    "lags = pd.concat([pd.Series(df_sum[\"detrended\"]).shift(x) for x in range(0, NUM_LAGS)], axis=1).to_numpy()\n",
    "event_feats = np.concatenate([df_sum[\"event_next_day\"].to_numpy()[:,np.newaxis],\n",
    "                             df_sum[\"late_event\"].to_numpy()[:,np.newaxis],\n",
    "                             df_sum[\"really_late_event\"].to_numpy()[:,np.newaxis],\n",
    "                             df_sum[\"really_late_event_next_day\"].to_numpy()[:,np.newaxis]], axis=1)\n",
    "lags_event_feats = pd.concat([pd.Series(df_sum[\"event_next_day\"]).shift(x) for x in range(0,NUM_LAGS)],axis=1).to_numpy()\n",
    "event_texts = df_sum[\"event_next_day_desc\"].to_numpy()\n",
    "weather_feats = df_sum[['min_temp', u'max_temp', u'wind_speed',\n",
    "       u'wind_gust', u'visibility', u'pressure', u'precipitation',\n",
    "       u'snow_depth', u'fog', u'rain_drizzle', u'snow_ice', u'thunder']].to_numpy()\n",
    "preds = pd.Series(df_sum[\"detrended\"]).shift(-1).to_numpy()\n",
    "trends = df_sum[\"trend\"].to_numpy()\n",
    "stds = df_sum[\"std\"].to_numpy()\n",
    "\n",
    "lags = lags[NUM_LAGS:-1,:]\n",
    "event_feats = event_feats[NUM_LAGS:-1,:]\n",
    "lags_event_feats = lags_event_feats[NUM_LAGS:-1,:]\n",
    "event_texts = event_texts[NUM_LAGS:-1]\n",
    "weather_feats = weather_feats[NUM_LAGS:-1,:]\n",
    "preds = preds[NUM_LAGS:-1]\n",
    "trends = trends[NUM_LAGS:-1]\n",
    "stds = stds[NUM_LAGS:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train/val/test split...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading train/val/test split...\")\n",
    "\n",
    "i_train = 365*2 # 2013 and 2014\n",
    "i_val = 365*3\n",
    "i_test = -1 # 2015 and 2016 (everything else)\n",
    "\n",
    "lags_train = lags[:i_train,:] # time series lags\n",
    "event_feats_train = event_feats[:i_train,:] # event/no_event\n",
    "lags_event_feats_train = lags_event_feats[:i_train,:] # lags for event/no_event\n",
    "event_texts_train = event_texts[:i_train] # event text descriptions\n",
    "weather_feats_train = weather_feats[:i_train,:] # weather data\n",
    "y_train = preds[:i_train] # target values\n",
    "\n",
    "lags_val = lags[i_train:i_val,:] # time series lags\n",
    "event_feats_val = event_feats[i_train:i_val,:] # event/no_event\n",
    "lags_event_feats_val = lags_event_feats[i_train:i_val,:] # lags for event/no_event\n",
    "event_texts_val = event_texts[i_train:i_val] # event text descriptions\n",
    "weather_feats_val = weather_feats[i_train:i_val,:] # weather data\n",
    "y_val = preds[i_train:i_val] # target values\n",
    "\n",
    "lags_test = lags[i_val:i_test,:]\n",
    "event_feats_test = event_feats[i_val:i_test,:]\n",
    "lags_event_feats_test = lags_event_feats[i_val:i_test,:]\n",
    "event_texts_test = event_texts[i_val:i_test]\n",
    "weather_feats_test = weather_feats[i_val:i_test,:]\n",
    "y_test = preds[i_val:i_test]\n",
    "trend_test = trends[i_val:i_test]\n",
    "std_test = stds[i_val:i_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(trues, predicted):\n",
    "    corr = np.corrcoef(predicted, trues)[0,1]\n",
    "    mae = np.mean(np.abs(predicted - trues))\n",
    "    rae = np.sum(np.abs(predicted - trues)) / np.sum(np.abs(trues - np.mean(trues)))\n",
    "    rmse = np.sqrt(np.mean((predicted - trues)**2))\n",
    "    rrse = np.sqrt(np.sum((predicted - trues)**2) / np.sum((trues - np.mean(trues))**2))\n",
    "    mape = np.mean(np.abs((predicted - trues) / trues)) * 100\n",
    "    r2 = max(0, 1 - np.sum((predicted - trues)**2) / np.sum((trues - np.mean(trues))**2))\n",
    "    return corr, mae, rae, rmse, rrse, mape, r2\n",
    "\n",
    "\n",
    "def compute_error_filtered(trues, predicted, filt):\n",
    "    trues = trues[filt]\n",
    "    predicted = predicted[filt]\n",
    "    corr = np.corrcoef(predicted, trues)[0,1]\n",
    "    mae = np.mean(np.abs(predicted - trues))\n",
    "    mse = np.mean((predicted - trues)**2)\n",
    "    rae = np.sum(np.abs(predicted - trues)) / np.sum(np.abs(trues - np.mean(trues)))\n",
    "    rmse = np.sqrt(np.mean((predicted - trues)**2))\n",
    "    r2 = max(0, 1 - np.sum((trues-predicted)**2) / np.sum((trues - np.mean(trues))**2))\n",
    "    return corr, mae, rae, rmse, rrse, mape, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP (just lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running MLP with just lags...\n",
      "Total number of iterations:   500\n",
      "Best loss at iteratation:     342    Best: 0.47571131587028503\n",
      "Best val_loss at iteratation: 292    Best: 0.39400947093963623\n",
      "MAE:  182.178\tRMSE: 250.563\tR2:   0.444\n"
     ]
    }
   ],
   "source": [
    "def build_model(num_inputs, num_lags, num_preds):\n",
    "    input_lags = Input(shape=(num_lags,))\n",
    "    \n",
    "    x = input_lags\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units=100, activation=\"tanh\", kernel_regularizer=keras.regularizers.l2(0.05))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(units=num_preds)(x)\n",
    "    \n",
    "    model = Model(input_lags, preds)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    \n",
    "    return model, input_lags, preds\n",
    "\n",
    "\n",
    "print(\"\\nrunning MLP with just lags...\")\n",
    "\n",
    "# checkpoint best model\n",
    "checkpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "model, input_lags, preds = build_model(1, NUM_LAGS, 1)\n",
    "model.fit(\n",
    "    np.concatenate([lags_train], axis=1),\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=500,\n",
    "    validation_data=(np.concatenate([lags_val], axis=1), y_val),\n",
    "    callbacks=[checkpoint],\n",
    "    verbose=0)   \n",
    "\n",
    "print(\"Total number of iterations:  \", len(model.history.history[\"loss\"]))\n",
    "print(\"Best loss at iteratation:    \", np.argmin(model.history.history[\"loss\"]), \"   Best:\", np.min(model.history.history[\"loss\"]))\n",
    "print(\"Best val_loss at iteratation:\", np.argmin(model.history.history[\"val_loss\"]), \"   Best:\", np.min(model.history.history[\"val_loss\"]))\n",
    "\n",
    "# load weights\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "# make predictions\n",
    "preds_lstm = model.predict(np.concatenate([lags_test[:,:]], axis=1))\n",
    "preds_lstm = preds_lstm[:,0] * std_test + trend_test\n",
    "y_true = y_test * std_test + trend_test\n",
    "corr, mae, rae, rmse, rrse, mape, r2 = compute_error(y_true, preds_lstm)\n",
    "print(\"MAE:  %.3f\\tRMSE: %.3f\\tR2:   %.3f\" % (mae, rmse, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP lags + weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running MLP with lags + weather...\n",
      "Total number of iterations:   500\n",
      "Best loss at iteratation:     302    Best: 0.47776007652282715\n",
      "Best val_loss at iteratation: 462    Best: 0.39925527572631836\n",
      "MAE:  183.752\tRMSE: 252.426\tR2:   0.436\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nrunning MLP with lags + weather...\")\n",
    "\n",
    "# checkpoint best model\n",
    "checkpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "model, input_lags, preds = build_model(1, NUM_LAGS+len(sel), 1)\n",
    "model.fit(\n",
    "    #lags_train,\n",
    "    np.concatenate([lags_train, weather_feats_train[:,sel]], axis=1),\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=500,\n",
    "    validation_data=(np.concatenate([lags_val, weather_feats_val[:,sel]], axis=1), y_val),\n",
    "    callbacks=[checkpoint],\n",
    "    verbose=0)   \n",
    "\n",
    "print(\"Total number of iterations:  \", len(model.history.history[\"loss\"]))\n",
    "print(\"Best loss at iteratation:    \", np.argmin(model.history.history[\"loss\"]), \"   Best:\", np.min(model.history.history[\"loss\"]))\n",
    "print(\"Best val_loss at iteratation:\", np.argmin(model.history.history[\"val_loss\"]), \"   Best:\", np.min(model.history.history[\"val_loss\"]))\n",
    "\n",
    "# load weights\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "# make predictions\n",
    "preds_lstm = model.predict(np.concatenate([lags_test[:,:], weather_feats_test[:,sel]], axis=1))\n",
    "preds_lstm = preds_lstm[:,0] * std_test + trend_test\n",
    "corr, mae, rae, rmse, rrse, mape, r2 = compute_error(y_true, preds_lstm)\n",
    "print(\"MAE:  %.3f\\tRMSE: %.3f\\tR2:   %.3f\" % (mae, rmse, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with weather + events information (no text) + late + event_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running MLP with lags + weather + event + late + event lags...\n",
      "Total number of iterations:   500\n",
      "Best loss at iteratation:     326    Best: 0.4455840289592743\n",
      "Best val_loss at iteratation: 476    Best: 0.3511611223220825\n",
      "6/6 - 0s - loss: 0.3940 - 49ms/epoch - 8ms/step\n",
      "0.3940379023551941\n",
      "MAE:  164.943\tRMSE: 241.907\tR2:   0.482\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nrunning MLP with lags + weather + event + late + event lags...\")\n",
    "\n",
    "def build_model_events(num_inputs, num_lags, num_feat, num_preds):\n",
    "    input_lags = Input(shape=(num_lags,))\n",
    "    input_events = Input(shape=(num_feat,))\n",
    "    \n",
    "    feat = Concatenate(axis=1)([input_lags, input_events])\n",
    "    \n",
    "    x = feat\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units=100, activation=\"tanh\", kernel_regularizer=keras.regularizers.l2(0.05))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    preds = Dense(units=num_preds)(x)\n",
    "    preds = Activation(\"linear\")(preds)\n",
    "    \n",
    "    model = Model([input_lags, input_events], preds)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    \n",
    "    return model, input_lags, preds\n",
    "\n",
    "# checkpoint best model\n",
    "checkpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "# fit model to the mean\n",
    "model, input_lags, preds = build_model_events(1, NUM_LAGS+len(sel), 4+len(sel2), 1)\n",
    "model.fit(\n",
    "    [np.concatenate([lags_train, weather_feats_train[:,sel]], axis=1), \n",
    "     np.concatenate([event_feats_train[:,:], lags_event_feats_train[:,sel2]], axis=1)],\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=500,\n",
    "    validation_data=([np.concatenate([lags_val, weather_feats_val[:,sel]], axis=1), \n",
    "                      np.concatenate([event_feats_val[:,:], lags_event_feats_val[:,sel2]], axis=1)], y_val),\n",
    "    callbacks=[checkpoint],\n",
    "    verbose=0)   \n",
    "\n",
    "print(\"Total number of iterations:  \", len(model.history.history[\"loss\"]))\n",
    "print(\"Best loss at iteratation:    \", np.argmin(model.history.history[\"loss\"]), \"   Best:\", np.min(model.history.history[\"loss\"]))\n",
    "print(\"Best val_loss at iteratation:\", np.argmin(model.history.history[\"val_loss\"]), \"   Best:\", np.min(model.history.history[\"val_loss\"]))\n",
    "\n",
    "# load weights\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "print(model.evaluate([np.concatenate([lags_test[:,:], weather_feats_test[:,sel]], axis=1), \n",
    "                      np.concatenate([event_feats_test[:,:], lags_event_feats_test[:,sel2]], axis=1)], \n",
    "                      y_test, verbose=2))\n",
    "\n",
    "# make predictions\n",
    "preds_lstm = model.predict([np.concatenate([lags_test[:,:], weather_feats_test[:,sel]], axis=1), \n",
    "                            np.concatenate([event_feats_test[:,:], lags_event_feats_test[:,sel2]], axis=1)])\n",
    "preds_lstm = preds_lstm[:,0] * std_test + trend_test\n",
    "corr, mae, rae, rmse, rrse, mape, r2 = compute_error(y_true, preds_lstm)\n",
    "print(\"MAE:  %.3f\\tRMSE: %.3f\\tR2:   %.3f\" % (mae, rmse, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with weather + events information (no text) + event_lags + TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "preparing word embeddings for NNs with text...\n",
      "Found 965 unique tokens.\n",
      "Shape of train tensor: (730, 350)\n",
      "Shape of val tensor: (365, 350)\n",
      "Shape of test tensor: (170, 350)\n",
      "Preparing embedding matrix.\n",
      "\n",
      "running MLP with lags + weather + events + late + text...\n",
      "text_embedding: KerasTensor(type_spec=TensorSpec(shape=(None, 180), dtype=tf.float32, name=None), name='flatten_5/Reshape:0', description=\"created by layer 'flatten_5'\")\n",
      "temp1: KerasTensor(type_spec=TensorSpec(shape=(None, 180, 1), dtype=tf.float32, name=None), name='permute_10/transpose:0', description=\"created by layer 'permute_10'\")\n",
      "x_lags: KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name=None), name='batch_normalization_28/batchnorm/add_1:0', description=\"created by layer 'batch_normalization_28'\")\n",
      "temp2: KerasTensor(type_spec=TensorSpec(shape=(None, 180, 100), dtype=tf.float32, name=None), name='dropout_36/Identity:0', description=\"created by layer 'dropout_36'\")\n",
      "concatenated: KerasTensor(type_spec=TensorSpec(shape=(None, 180, 101), dtype=tf.float32, name=None), name='concatenate_15/concat:0', description=\"created by layer 'concatenate_15'\")\n",
      "after tanh: KerasTensor(type_spec=TensorSpec(shape=(None, 180, 1), dtype=tf.float32, name=None), name='dense_25/Tanh:0', description=\"created by layer 'dense_25'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 180), dtype=tf.float32, name=None), name='batch_normalization_30/batchnorm/add_1:0', description=\"created by layer 'batch_normalization_30'\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input has undefined rank. Received: input_shape=<unknown>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [100]\u001b[0m, in \u001b[0;36m<cell line: 141>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights.best.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# fit model to the mean\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m model, input_lags, preds \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_LAGS\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    143\u001b[0m     [np\u001b[38;5;241m.\u001b[39mconcatenate([lags_train, weather_feats_train[:,sel]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \n\u001b[1;32m    144\u001b[0m      np\u001b[38;5;241m.\u001b[39mconcatenate([event_feats_train[:,:]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint],\n\u001b[1;32m    154\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)   \n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of iterations:  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "Input \u001b[0;32mIn [100]\u001b[0m, in \u001b[0;36mbuild_model_text\u001b[0;34m(num_inputs, num_lags, num_feat, num_preds)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#print fail\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#attention_probs = Dense(180, activation='softmax', name='attention_vec')(text_embedding)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#attention_mul = merge([text_embedding, attention_probs], output_shape=180, name='attention_mul', mode='mul')\u001b[39;00m\n\u001b[1;32m    112\u001b[0m attention_mul \u001b[38;5;241m=\u001b[39m Concatenate([text_embedding, attention_probs])\n\u001b[0;32m--> 113\u001b[0m attention_mul \u001b[38;5;241m=\u001b[39m \u001b[43mBatchNormalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mul\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#attention_mul = Dropout(0.5)(attention_mul)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m feat \u001b[38;5;241m=\u001b[39m Concatenate(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)([x_lags, attention_mul])\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/layers/normalization/batch_normalization.py:297\u001b[0m, in \u001b[0;36mBatchNormalizationBase.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    295\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensorShape(input_shape)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m input_shape\u001b[38;5;241m.\u001b[39mndims:\n\u001b[0;32m--> 297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    298\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput has undefined rank. Received: input_shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    299\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_shape)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Convert axis to list and resolve negatives\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Input has undefined rank. Received: input_shape=<unknown>."
     ]
    }
   ],
   "source": [
    "print(\"\\npreparing word embeddings for NNs with text...\")\n",
    "\n",
    "# Build index mapping words in the embeddings set to their embedding vector\n",
    "embeddings_index = {}\n",
    "# f = open(GLOVE_DIR + 'glove.6B.%dd.txt' % (EMBEDDING_DIM,))\n",
    "f = open('glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# Vectorize the text samples into a 2D integer tensor and pad sequences\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(event_texts)\n",
    "sequences_train = tokenizer.texts_to_sequences(event_texts_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(event_texts_val)\n",
    "sequences_test = tokenizer.texts_to_sequences(event_texts_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_val = pad_sequences(sequences_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train tensor:', data_train.shape)\n",
    "print('Shape of val tensor:', data_val.shape)\n",
    "print('Shape of test tensor:', data_test.shape)\n",
    "\n",
    "# Prepare embedding matrix\n",
    "print('Preparing embedding matrix.')\n",
    "num_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    #print i\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "def build_model_text(num_inputs, num_lags, num_feat, num_preds):\n",
    "    input_lags = Input(shape=(num_lags,))\n",
    "    input_events = Input(shape=(num_feat,))\n",
    "    \n",
    "    x_lags = Concatenate(axis=1)([input_lags, input_events])\n",
    "    #x_lags = BatchNormalization()(x_lags)\n",
    "    \n",
    "    x = x_lags\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units=100, activation=\"tanh\", kernel_regularizer=keras.regularizers.l2(0.05))(x)\n",
    "    #x = Dense(units=50, activation=\"tanh\", kernel_regularizer=keras.regularizers.l2(0.1))(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dense(units=50, activation=\"tanh\", kernel_regularizer=keras.regularizers.l2(0.1))(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x_lags = BatchNormalization()(x)\n",
    "    #x_lags = x\n",
    "    \n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(50, 3, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv1D(30, 3, activation='relu')(x)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv1D(30, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #x = Conv1D(50, 5, activation='relu')(x)\n",
    "    #x = MaxPooling1D(5)(x)\n",
    "    text_embedding = Flatten()(x)\n",
    "    #text_embedding = Dropout(0.5)(text_embedding)\n",
    "    #text_embedding = Dense(units=100, activation='relu')(text_embedding)\n",
    "    #text_embedding = Dropout(0.5)(text_embedding)\n",
    "    \n",
    "    print(\"text_embedding:\", text_embedding)\n",
    "    #temp1 = Permute([1,2])(RepeatVector(180)(BatchNormalization()(text_embedding)))\n",
    "    temp1 = Reshape((1,180))(text_embedding)\n",
    "    temp1 = Permute([2,1])(temp1)\n",
    "    print(\"temp1:\", temp1)\n",
    "    \n",
    "    print(\"x_lags:\", x_lags)\n",
    "    temp2 = Permute([1,2])(RepeatVector(180)(BatchNormalization()(x_lags)))\n",
    "    temp2 = Dropout(0.5)(temp2)\n",
    "    print(\"temp2:\", temp2)\n",
    "    \n",
    "    temp = Concatenate(axis=2)([temp1, temp2])\n",
    "    print(\"concatenated:\", temp)\n",
    "    temp = Dense(1, activation=\"tanh\")(temp)\n",
    "    #temp = Permute([2,1])(temp)\n",
    "    print(\"after tanh:\", temp)\n",
    "    temp = Reshape((180,))(temp)\n",
    "    temp = BatchNormalization()(temp)\n",
    "    print(temp)\n",
    "    attention_probs = Activation(\"softmax\")(temp)\n",
    "    #print fail\n",
    "\n",
    "    #attention_probs = Dense(180, activation='softmax', name='attention_vec')(text_embedding)\n",
    "    #attention_mul = merge([text_embedding, attention_probs], output_shape=180, name='attention_mul', mode='mul')\n",
    "    attention_mul = Concatenate([text_embedding, attention_probs])\n",
    "    attention_mul = BatchNormalization()(attention_mul)\n",
    "    #attention_mul = Dropout(0.5)(attention_mul)\n",
    "    \n",
    "    feat = Concatenate(axis=1)([x_lags, attention_mul])\n",
    "    \n",
    "    feat = BatchNormalization()(feat)\n",
    "    #feat = Dense(units=50, activation='relu')(feat)\n",
    "    #feat = Dropout(0.5)(feat)\n",
    "    \n",
    "    preds = Dense(units=num_preds)(feat)\n",
    "    #preds = Dense(units=num_preds, kernel_regularizer=keras.regularizers.l2(0.2))(feat)\n",
    "    preds = Activation(\"linear\")(preds)\n",
    "    \n",
    "    model = Model([input_lags, input_events, sequence_input], preds)\n",
    "    \n",
    "    rmsp = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    #model.compile(loss=\"mse\", optimizer=rmsp)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    \n",
    "    return model, input_lags, preds\n",
    "\n",
    "\n",
    "print(\"\\nrunning MLP with lags + weather + events + late + text...\")\n",
    "\n",
    "# checkpoint best model\n",
    "checkpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "# fit model to the mean\n",
    "model, input_lags, preds = build_model_text(1, NUM_LAGS+len(sel), 4, 1)\n",
    "model.fit(\n",
    "    [np.concatenate([lags_train, weather_feats_train[:,sel]], axis=1), \n",
    "     np.concatenate([event_feats_train[:,:]], axis=1),\n",
    "     data_train],\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=700,\n",
    "    #validation_split=0.2,\n",
    "    validation_data=([np.concatenate([lags_val, weather_feats_val[:,sel]], axis=1), \n",
    "                      np.concatenate([event_feats_val[:,:]], axis=1),\n",
    "                      data_val], y_val),\n",
    "    callbacks=[checkpoint],\n",
    "    verbose=0)   \n",
    "\n",
    "print(\"Total number of iterations:  \", len(model.history.history[\"loss\"]))\n",
    "print(\"Best loss at iteratation:    \", np.argmin(model.history.history[\"loss\"]), \"   Best:\", np.min(model.history.history[\"loss\"]))\n",
    "print(\"Best val_loss at iteratation:\", np.argmin(model.history.history[\"val_loss\"]), \"   Best:\", np.min(model.history.history[\"val_loss\"]))\n",
    "\n",
    "# load weights\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "print(model.evaluate([np.concatenate([lags_test[:,:], weather_feats_test[:,sel]], axis=1), \n",
    "                      np.concatenate([event_feats_test[:,:]], axis=1),\n",
    "                      data_test],\n",
    "                      y_test, verbose=2))\n",
    "\n",
    "# make predictions\n",
    "preds_lstm = model.predict([np.concatenate([lags_test[:,:], weather_feats_test[:,sel]], axis=1), \n",
    "                            np.concatenate([event_feats_test[:,:]], axis=1),\n",
    "                            data_test])\n",
    "preds_lstm = preds_lstm[:,0] * std_test + trend_test\n",
    "y_true = y_test * std_test + trend_test\n",
    "corr, mae, rae, rmse, rrse, mape, r2 = compute_error(y_true, preds_lstm)\n",
    "print(\"MAE:  %.3f\\tRMSE: %.3f\\tR2:   %.3f\" % (mae, rmse, r2))\n",
    "\n",
    "\n",
    "# ---------------------------------------- MLP with weather + events information (no text) + event_lags + TEXT\n",
    "\n",
    "def build_model_text_v2(num_inputs, num_lags, num_feat, num_preds):\n",
    "    input_lags = Input(shape=(num_lags,))\n",
    "    input_events = Input(shape=(num_feat,))\n",
    "    \n",
    "    x_lags = Concatenate(axis=1)([input_lags, input_events])\n",
    "    #x_lags = BatchNormalization()(x_lags)\n",
    "    \n",
    "    x = x_lags\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units=100, activation=\"tanh\", kernel_regularizer=keras.regularizers.l2(0.05))(x)\n",
    "    #x = Dense(units=50, activation=\"tanh\", kernel_regularizer=keras.regularizers.l2(0.1))(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dense(units=50, activation=\"tanh\", kernel_regularizer=keras.regularizers.l2(0.1))(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x_lags = BatchNormalization()(x)\n",
    "    #x_lags = x\n",
    "    \n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(50, 3, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv1D(30, 3, activation='relu')(x)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv1D(30, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #x = Conv1D(50, 5, activation='relu')(x)\n",
    "    #x = MaxPooling1D(5)(x)\n",
    "    text_embedding = Flatten()(x)\n",
    "    #text_embedding = Dropout(0.5)(text_embedding)\n",
    "    #text_embedding = Dense(units=100, activation='relu')(text_embedding)\n",
    "    #text_embedding = Dropout(0.5)(text_embedding)\n",
    "    \n",
    "    print(\"text_embedding:\", text_embedding)\n",
    "    #temp1 = Permute([1,2])(RepeatVector(180)(BatchNormalization()(text_embedding)))\n",
    "    temp1 = Reshape((1,180))(text_embedding)\n",
    "    temp1 = Permute([2,1])(temp1)\n",
    "    print(\"temp1:\", temp1)\n",
    "    \n",
    "    print(\"x_lags:\", x_lags)\n",
    "    temp2 = Permute([1,2])(RepeatVector(180)(BatchNormalization()(x_lags)))\n",
    "    temp2 = Dropout(0.5)(temp2)\n",
    "    print(\"temp2:\", temp2)\n",
    "    \n",
    "    temp = Concatenate(axis=2)([temp1, temp2])\n",
    "    print(\"concatenated:\", temp)\n",
    "    temp = Dense(1, activation=\"tanh\")(temp)\n",
    "    #temp = Permute([2,1])(temp)\n",
    "    print(\"after tanh:\", temp)\n",
    "    temp = Reshape((180,))(temp)\n",
    "    temp = BatchNormalization()(temp)\n",
    "    print(temp)\n",
    "    attention_probs = Activation(\"softmax\")(temp)\n",
    "    #print fail\n",
    "\n",
    "    #attention_probs = Dense(180, activation='softmax', name='attention_vec')(text_embedding)\n",
    "    #attention_mul = merge([text_embedding, attention_probs], output_shape=180, name='attention_mul', mode='mul')\n",
    "    attention_mul = Concatenate([text_embedding, attention_probs])\n",
    "    attention_mul = BatchNormalization()(attention_mul)\n",
    "    #attention_mul = Dropout(0.5)(attention_mul)\n",
    "    \n",
    "    feat = Concatenate(axis=1)([x_lags, attention_mul])\n",
    "    \n",
    "    feat = BatchNormalization()(feat)\n",
    "    #feat = Dense(units=50, activation='relu')(feat)\n",
    "    #feat = Dropout(0.5)(feat)\n",
    "    \n",
    "    preds = Dense(units=num_preds)(feat)\n",
    "    #preds = Dense(units=num_preds, kernel_regularizer=keras.regularizers.l2(0.2))(feat)\n",
    "    preds = Activation(\"linear\")(preds)\n",
    "    \n",
    "    model = Model([input_lags, input_events, sequence_input], preds)\n",
    "    \n",
    "    rmsp = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    #model.compile(loss=\"mse\", optimizer=rmsp)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    \n",
    "    return model, input_lags, preds\n",
    "\n",
    "\n",
    "print(\"\\nrunning MLP with lags + weather + events + late + event_lags + text...\")\n",
    "\n",
    "# checkpoint best model\n",
    "checkpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "# fit model to the mean\n",
    "model, input_lags, preds = build_model_text_v2(1, NUM_LAGS+len(sel), 4+len(sel2), 1)\n",
    "model.fit(\n",
    "    [np.concatenate([lags_train, weather_feats_train[:,sel]], axis=1), \n",
    "     np.concatenate([event_feats_train[:,:], lags_event_feats_train[:,sel2]], axis=1),\n",
    "     data_train],\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=700,\n",
    "    #validation_split=0.2,\n",
    "    validation_data=([np.concatenate([lags_val, weather_feats_val[:,sel]], axis=1), \n",
    "                      np.concatenate([event_feats_val[:,:], lags_event_feats_val[:,sel2]], axis=1),\n",
    "                      data_val], y_val),\n",
    "    callbacks=[checkpoint],\n",
    "    verbose=0)   \n",
    "\n",
    "print(\"Total number of iterations:  \", len(model.history.history[\"loss\"]))\n",
    "print(\"Best loss at iteratation:    \", np.argmin(model.history.history[\"loss\"]), \"   Best:\", np.min(model.history.history[\"loss\"]))\n",
    "print(\"Best val_loss at iteratation:\", np.argmin(model.history.history[\"val_loss\"]), \"   Best:\", np.min(model.history.history[\"val_loss\"]))\n",
    "\n",
    "# load weights\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "print(model.evaluate([np.concatenate([lags_test[:,:], weather_feats_test[:,sel]], axis=1), \n",
    "                      np.concatenate([event_feats_test[:,:], lags_event_feats_test[:,sel2]], axis=1),\n",
    "                      data_test],\n",
    "                      y_test, verbose=2))\n",
    "\n",
    "# make predictions\n",
    "preds_lstm = model.predict([np.concatenate([lags_test[:,:], weather_feats_test[:,sel]], axis=1), \n",
    "                            np.concatenate([event_feats_test[:,:], lags_event_feats_test[:,sel2]], axis=1),\n",
    "                            data_test])\n",
    "preds_lstm = preds_lstm[:,0] * std_test + trend_test\n",
    "corr, mae, rae, rmse, rrse, mape, r2 = compute_error(y_true, preds_lstm)\n",
    "print(\"MAE:  %.3f\\tRMSE: %.3f\\tR2:   %.3f\" % (mae, rmse, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
